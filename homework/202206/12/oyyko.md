[TOC]

## CACHE 简介

**Cache是用来对内存数据的缓存。**

**CPU要访问的数据在Cache中有缓存，称为“命中” (Hit)，反之则称为“缺失” (Miss)。**

**CPU访问它的速度介于寄存器与内存之间（数量级的差别）。实现Cache的花费介于寄存器与内存之间。**

现在 CPU 的 Cache 又被细分了几层，常见的有 L1 Cache, L2 Cache, L3 Cache，其读写延迟依次增加，实现的成本依次降低。

引入 Cache 的理论基础是**程序局部性原理**，包括时间局部性和空间局部性。即最近被CPU访问的数据，短期内CPU 还要访问（时间）；被 CPU 访问的数据附近的数据，CPU 短期内还要访问（空间）。因此如果将刚刚访问过的数据缓存在Cache中，那下次访问时，可以直接从Cache中取，其速度可以得到数量级的提高。

从延迟上看，做一次乘法一般只要三个周期，而做一次CPU的内存访问需要167个cycle，如果需要提升程序性能，减少CPU的memory访问至关重要。因此，需要采用容量小但是更快的存储器（cache）

Cache Line可以简单的理解为CPU Cache中的最小缓存单位。
内存和高速缓存之间或高速缓存之间的数据移动不是以单个字节或甚至word完成的。
相反，移动的最小数据单位称为缓存行，有时称为缓存块
目前主流的CPU Cache的Cache Line大小都是64Bytes。假设我们有一个512字节的一级缓存，那么按照64B的缓存单位大小来算，这个一级缓存所能存放的缓存个数就是512/64 = 8个。
查看cache line大小
cat /sys/devices/system/cpu/cpu1/cache/index0/coherency_line_size



Cache写机制分为write through和write back两种。

Write-through- Write is done synchronously both to the cache and to the backing store.
Write-back (or Write-behind) - Writing is done only to the cache. A modified cache block is written back to the store, just before it is replaced.

Write-through（直写模式）在数据更新时，同时写入缓存Cache和后端存储。此模式的优点是操作简单；缺点是因为数据修改需要同时写入存储，数据写入速度较慢。

Write-back（回写模式）在数据更新时只写入缓存Cache。只在数据被替换出缓存时，被修改的缓存数据才会被写到后端存储。此模式的优点是数据写入速度快，因为不需要写存储；缺点是一旦更新后的数据未被写入存储时出现系统掉电的情况，数据将无法找回



**cache 一致性**

多个处理器对某个内存块同时读写，会引起冲突的问题，这也被称为Cache一致性问题。
Cache一致性问题出现的原因是在一个多处理器系统中，多个处理器核心都能够独立地执行计算机指令，从而有可能同时对某个内存块进行读写操作，并且由于我们之前提到的回写和直写的Cache策略，导致一个内存块同时可能有多个备份，有的已经写回到内存中，有的在不同的处理器核心的一级、二级Cache中。由于Cache缓存的原因，我们不知道数据写入的时序性，因而也不知道哪个备份是最新的。还有另外一个一种可能，假设有两个线程A和B共享一个变量，当线程A处理完一个数据之后，通过这个变量通知线程B，然后线程B对这个数据接着进行处理，如果两个线程运行在不同的处理器核心上，那么运行线程B的处理器就会不停地检查这个变量，而这个变量存储在本地的Cache中，因此就会发现这个值总也不会发生变化。

为了正确性，一旦一个核心更新了内存中的内容，硬件就必须要保证其他的核心能够读到更新后的数据。目前大多数硬件采用的策略或协议是MESI或基于MESI的变种：
M代表更改（modified），表示缓存中的数据已经更改，在未来的某个时刻将会写入内存；
E代表排除（exclusive），表示缓存的数据只被当前的核心所缓存；
S代表共享（shared），表示缓存的数据还被其他核心缓存；
I代表无效（invalid），表示缓存中的数据已经失效，即其他核心更改了数据。



Cache工作原理要求它尽量保存最新数据，当从主存向Cache传送一个新块，而Cache中可用位置已被占满时，就会产生Cache替换的问题。
常用的替换算法有下面三种。
（1） LFU
LFU（Least Frequently Used，最不经常使用）算法将一段时间内被访问次数最少的那个块替换出去。每块设置一个计数器，从0开始计数，每访问一次，被访块的计数器就增1。当需要替换时，将计数值最小的块换出，同时将所有块的计数器都清零。
这种算法将计数周期限定在对这些特定块两次替换之间的间隔时间内，不能严格反映近期访问情况，新调入的块很容易被替换出去。
（2）LRU
LRU（Least Recently Used，近期最少使用）算法是把CPU近期最少使用的块替换出去。这种替换方法需要随时记录Cache中各块的使用情况，以便确定哪个块是近期最少使用的块。每块也设置一个计数器，Cache每命中一次，命中块计数器清零，其他各块计数器增1。当需要替换时，将计数值最大的块换出。
LRU算法相对合理，但实现起来比较复杂，系统开销较大。这种算法保护了刚调入Cache的新数据块，具有较高的命中率。LRU算法不能肯定调出去的块近期不会再被使用，所以这种替换算法不能算作最合理、最优秀的算法。但是研究表明，采用这种算法可使Cache的命中率达到90%左右。
（3） 随机替换
最简单的替换算法是随机替换。随机替换算法完全不管Cache的情况，简单地根据一个随机数选择一块替换出去。随机替换算法在硬件上容易实现，且速度也比前两种算法快。缺点则是降低了命中率和Cache工作效率。

## Cache miss

当运算器需要从存储器中提取数据时，它首先在最高级的cache中寻找然后在次高级的cache中寻找。如果在cache中找到，则称为命中hit；反之，则称为不命中miss。

cache misses的种类：

（1）cold misses：不可避免。若K级cache空，则必发生cache miss，空的cache称为cold cache，这种cache misses称为compulsory misses或者cold misses。当cache已被warmed up则一般不会再发生cold misses。

（2）conflict misses：多个K+1级的blocks被映射到K级中同一个block。这一点关系到对于程序员而言能否写出cache友好代码。

（3）程序常会分阶段执行（例如循环：内层、外层），每个阶段会取cache blocks的固定几个块，这几个块所构成的集合称为working set。 当working set超过cache大小时所发生的miss称为capacity misses。

从cache指令上做优化：简化调用关系，减少冗余代码（即不是必须存在的的代码），减小代码量，减少不必要的调用；

从数据cache上做优化：即减少cache miss的次数。



通过软件优化（Complier和手工）来减少Cache miss.
1.指令
~.对存储访问重排序，因而可以减少冲突失效
~.进行Profiling来检测冲突

2.数据
~.合并数组(data merge):通过将两个独立数组合并为一个复合元素的数组来改进空间局部性
~.循环交换(loop interchange): 通过改变循环嵌套来按序访问存储器中存储的数据
~.循环合并(loop fusion): 将两个具有相同循环类型且有一些变量重叠的独立循环合并
~.块化(blocking):通过不断使用一些数据块（而不是完整地遍历一行和一列）来改进时间局部性
