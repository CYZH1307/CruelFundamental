# Kafka保持数据完整性 #
Kafka丢失数据一般会在三个阶段，按时序来说就是：a.生产端把数据传到Kafka时、b.Kafka转发数据时、c.消费端开始对数据操作后。
a.
  1.重传机制：若网络可修复，则间隔一段时间reforward_T后重传，重传将会尝试一定次数reforward_N。这两个变量大小依网络状况而定。若网络当前不可修复，则保存在Kafka格式所指定的log中持久化处理，待修复后重传  
  2.get机制：需要get到大于一定比例的消费端副本都发出确认后才认为传输成功，不然就重传。  
b.
  1.ack机制：需要等待一定比例的消费端副本都发出确认之后才认为传输成功，若规定时间内确认的副本里面没有leader节点、或者leader节点传来故障信息，则从replica副本节点里面选举一个作为新的leader节点。  
c.
  1.offset机制：大段数据分为若干段从Kafka传到消费端，每一段都带有首位偏移量offset1和末位偏移量offset2，每读到一段都需要匹配上一段offset2和本段offset1是否一致。
